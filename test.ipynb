{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from libs.global_variable import ROOT_DIR\n",
    "\n",
    "path = ROOT_DIR + 'Data/BuFF/buff_release_rot_const/sequences/00096/shortshort_hips/shortshort_hips_000010_cano.npy'\n",
    "\n",
    "cano = np.load(path, allow_pickle=True).item()\n",
    "cano_verts = cano['cano_points']\n",
    "cano_normals = cano['cano_normals']\n",
    "valid_mask = cano['valid_mask']\n",
    "\n",
    "path_scan = ROOT_DIR + 'Data/BuFF/buff_release_rot_const/sequences/00096/shortshort_hips/shortshort_hips_000010.npy'\n",
    "\n",
    "scan = np.load(path_scan, allow_pickle=True).item()\n",
    "scan_verts = scan['points_posed_cloth']\n",
    "scan_normals = scan['normals_posed_cloth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_repeated_idx(num_points_input, num_points):\n",
    "\n",
    "    assert(num_points_input <= num_points)\n",
    "    idx_org = np.arange(0, num_points_input)\n",
    "\n",
    "    num_points_rest = num_points - num_points_input\n",
    "    if num_points_rest <= num_points_input:\n",
    "        idx_rest = np.random.choice(np.arange(0, num_points_input), num_points_rest, replace=False)\n",
    "    elif num_points_rest > num_points_input:\n",
    "        num_points_rest_1 = num_points_input\n",
    "        idx_rest_1 = np.random.choice(np.arange(0, num_points_input), num_points_rest_1, replace=False)\n",
    "        num_points_rest_2 = num_points_rest - num_points_rest_1\n",
    "        idx_rest_2 = np.random.choice(np.arange(0, num_points_input), num_points_rest_2, replace=False)\n",
    "        idx_rest = np.concatenate((idx_rest_1, idx_rest_2))\n",
    "\n",
    "    idx_list = np.concatenate((idx_org, idx_rest))\n",
    "\n",
    "    return idx_list\n",
    "\n",
    "idx_list = _get_repeated_idx(cano_verts.shape[0], 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24169,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cano_verts[idx_list].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_verts.transpose()[valid_mask][idx_list].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(cano_verts)\n",
    "pcd.normals = o3d.utility.Vector3dVector(cano_normals)\n",
    "\n",
    "# write point cloud\n",
    "o3d.io.write_point_cloud(\"visualization/00096_shortlong_hips_000216_cano.ply\", pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 569/569 [00:04<00:00, 137.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded, in total 569 train examples.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.dataloader_buff import DataLoader_Buff_depth\n",
    "\n",
    "splt_file = '/home/yuxuan/project/NeuralSurfaceField/assets/data_split/buff_female_train_val.pkl' \n",
    "subject_index_dict = {}\n",
    "subject_index_dict.update({\"00032_shortlong\": 0,\n",
    "                            \"00096_shortlong\": 1})\n",
    "train_dataset = DataLoader_Buff_depth(mode='train', batch_size=2, num_workers=4, split_file=splt_file, subject_index_dict=subject_index_dict)\n",
    "train_loader = train_dataset.get_loader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# write function which uses open3d to write point cloud\n",
    "def write_pcd(path, points, normals):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.normals = o3d.utility.Vector3dVector(normals)\n",
    "    o3d.io.write_point_cloud(path, pcd)\n",
    "\n",
    "path_cano = '/home/yuxuan/project/NeuralSurfaceField/visualization/cano.ply'\n",
    "path_posed = '/home/yuxuan/project/NeuralSurfaceField/visualization/posed.ply'\n",
    "\n",
    "# write cano point cloud\n",
    "write_pcd(path_cano, batch['cano_points'][0].cpu().numpy(), batch['cano_normals'][0].cpu().numpy())\n",
    "# write posed point cloud\n",
    "write_pcd(path_posed, batch['scan_points'][0].cpu().numpy().transpose(), batch['scan_normals'][0].cpu().numpy().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "dict_ = np.load('/home/yuxuan/project/NeuralSurfaceField/Data_scan/BuFF/buff_release/sequences/00159/shortshort_twist_tilt_left/shortshort_twist_tilt_left.000050.npz')\n",
    "\n",
    "# o3d point cloud visualization\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(dict_['v_posed'])\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "split_file = '/mnt/qb/work/ponsmoll/yxue80/project/NeuralSurfaceField/assets/data_split/buff_female_train_val.pkl'\n",
    "\n",
    "with open(split_file, \"rb\") as f:\n",
    "    split = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.global_variable import ROOT_DIR\n",
    "import os\n",
    "\n",
    "split_org = {\n",
    "    'train': [],\n",
    "    'val': []\n",
    "}\n",
    "\n",
    "for subj_train in split['train']:\n",
    "    split_org['train'].append(subj_train.replace('Data_male', 'Data'))\n",
    "\n",
    "for subj_val in split['val']:\n",
    "    split_org['val'].append(subj_val.replace('Data_male', 'Data'))\n",
    "\n",
    "\n",
    "pkl.dump(split_org, open(split_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30000, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['cano_points'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from: /mnt/qb/work/ponsmoll/yxue80/project/shapefusion/experiments/PoseImplicit_exp_id_808/checkpoints/checkpoint_epoch_2150.tar\n",
      "dict_keys(['epoch', 'optimizer_state_dict', 'scheduler_state_dict', 'feat_optimizer_state_dict', 'feat_scheduler_state_dict', 'pose_encoder_state_dict', 'shape_geometry_decoder_state_dict', 'conditional_ndf_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# load old checkpoint\n",
    "shapefusion_path = '/mnt/qb/work/ponsmoll/yxue80/project/shapefusion/experiments/PoseImplicit_exp_id_808'\n",
    "shapefusion_checkpoint_path = os.path.join(shapefusion_path, 'checkpoints', 'checkpoint_epoch_2150.tar')\n",
    "\n",
    "print('Loaded checkpoint from: {}'.format(shapefusion_checkpoint_path))\n",
    "checkpoint = torch.load(shapefusion_checkpoint_path)\n",
    "\n",
    "print(checkpoint.keys())\n",
    "\n",
    "# save to new checkpoint\n",
    "path = 'checkpoint_epoch_{}.tar'.format(checkpoint['epoch'])\n",
    "if not os.path.exists(path):\n",
    "    model_weights = {'epoch': checkpoint['epoch'],\n",
    "                    'optimizer_state_dict': checkpoint['optimizer_state_dict'], \n",
    "                    'scheduler_state_dict': checkpoint['scheduler_state_dict'],\n",
    "                    'feat_optimizer_state_dict': checkpoint['feat_optimizer_state_dict'],\n",
    "                    'feat_scheduler_state_dict': checkpoint['feat_scheduler_state_dict'],\n",
    "                    'pose_encoder_state_dict': checkpoint['pose_encoder_state_dict'],\n",
    "                    'nsf_decoder_state_dict': checkpoint['shape_geometry_decoder_state_dict']}\n",
    "    torch.save(model_weights, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 4.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with requires_grad enabled\n",
    "tensor = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=False)\n",
    "tensor2 = torch.tensor([1, 2, 4], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "mask = tensor > 2\n",
    "\n",
    "tensor[mask] = tensor2[mask]\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000.npz\n",
      "00002.npz\n",
      "00004.npz\n",
      "00006.npz\n",
      "00008.npz\n",
      "00010.npz\n",
      "00012.npz\n",
      "00014.npz\n",
      "00016.npz\n",
      "00018.npz\n",
      "00020.npz\n",
      "00022.npz\n",
      "00024.npz\n",
      "00026.npz\n",
      "00028.npz\n",
      "00030.npz\n",
      "00032.npz\n",
      "00034.npz\n",
      "00036.npz\n",
      "00038.npz\n",
      "00040.npz\n",
      "00042.npz\n",
      "00044.npz\n",
      "00046.npz\n",
      "00048.npz\n",
      "00050.npz\n",
      "00052.npz\n",
      "00054.npz\n",
      "00056.npz\n",
      "00058.npz\n",
      "00060.npz\n",
      "00062.npz\n",
      "00064.npz\n",
      "00066.npz\n",
      "00068.npz\n",
      "00070.npz\n",
      "00072.npz\n",
      "00074.npz\n",
      "00076.npz\n",
      "00078.npz\n",
      "00080.npz\n",
      "00082.npz\n",
      "00084.npz\n",
      "00086.npz\n",
      "00088.npz\n",
      "00090.npz\n",
      "00092.npz\n",
      "00094.npz\n",
      "00096.npz\n",
      "00098.npz\n",
      "00100.npz\n",
      "00102.npz\n",
      "00104.npz\n",
      "00106.npz\n",
      "00108.npz\n",
      "00110.npz\n",
      "00112.npz\n",
      "00114.npz\n",
      "00116.npz\n",
      "00118.npz\n",
      "00120.npz\n",
      "00122.npz\n",
      "00124.npz\n",
      "00126.npz\n",
      "00128.npz\n",
      "00130.npz\n",
      "00132.npz\n",
      "00134.npz\n",
      "00136.npz\n",
      "00138.npz\n",
      "00140.npz\n",
      "00142.npz\n",
      "00144.npz\n",
      "00146.npz\n",
      "00148.npz\n",
      "00150.npz\n",
      "00152.npz\n",
      "00154.npz\n",
      "00156.npz\n",
      "00158.npz\n",
      "00160.npz\n",
      "00162.npz\n",
      "00164.npz\n",
      "00166.npz\n",
      "00168.npz\n",
      "00170.npz\n",
      "00172.npz\n",
      "00174.npz\n",
      "00176.npz\n",
      "00178.npz\n",
      "00180.npz\n",
      "00182.npz\n",
      "00184.npz\n",
      "00186.npz\n",
      "00188.npz\n",
      "00190.npz\n",
      "00192.npz\n",
      "00194.npz\n",
      "00196.npz\n",
      "00198.npz\n",
      "00200.npz\n",
      "00202.npz\n",
      "00204.npz\n",
      "00206.npz\n",
      "00208.npz\n",
      "00210.npz\n",
      "00212.npz\n",
      "00214.npz\n",
      "00216.npz\n",
      "00218.npz\n",
      "00220.npz\n",
      "00222.npz\n",
      "00224.npz\n",
      "00226.npz\n",
      "00228.npz\n",
      "00230.npz\n",
      "00232.npz\n",
      "00234.npz\n",
      "00236.npz\n",
      "00238.npz\n",
      "00240.npz\n",
      "00242.npz\n",
      "00244.npz\n",
      "00246.npz\n",
      "00248.npz\n",
      "00250.npz\n",
      "00252.npz\n",
      "00254.npz\n",
      "00256.npz\n",
      "00258.npz\n",
      "00260.npz\n",
      "00262.npz\n",
      "00264.npz\n",
      "00266.npz\n",
      "00268.npz\n",
      "00270.npz\n",
      "00272.npz\n",
      "00274.npz\n",
      "00276.npz\n",
      "00278.npz\n",
      "00280.npz\n",
      "00282.npz\n",
      "00284.npz\n",
      "00286.npz\n",
      "00288.npz\n",
      "00290.npz\n",
      "00292.npz\n",
      "00294.npz\n",
      "00296.npz\n",
      "00298.npz\n",
      "00300.npz\n",
      "00302.npz\n",
      "00304.npz\n",
      "00306.npz\n",
      "00308.npz\n",
      "00310.npz\n",
      "00312.npz\n",
      "00314.npz\n",
      "00316.npz\n",
      "00318.npz\n",
      "00320.npz\n",
      "00322.npz\n",
      "00324.npz\n",
      "00326.npz\n",
      "00328.npz\n",
      "00330.npz\n",
      "00332.npz\n",
      "00334.npz\n",
      "00336.npz\n",
      "00338.npz\n",
      "00340.npz\n",
      "00342.npz\n",
      "00344.npz\n",
      "00346.npz\n",
      "00348.npz\n",
      "00350.npz\n",
      "00352.npz\n",
      "00354.npz\n",
      "00356.npz\n",
      "00358.npz\n",
      "00360.npz\n",
      "00362.npz\n",
      "00364.npz\n",
      "00366.npz\n",
      "00368.npz\n",
      "00370.npz\n",
      "00372.npz\n",
      "00374.npz\n",
      "00376.npz\n",
      "00378.npz\n",
      "00380.npz\n",
      "00382.npz\n",
      "00384.npz\n",
      "00386.npz\n",
      "00388.npz\n",
      "00390.npz\n",
      "00392.npz\n",
      "00394.npz\n",
      "00396.npz\n",
      "00398.npz\n",
      "00400.npz\n",
      "00402.npz\n",
      "00404.npz\n",
      "00406.npz\n",
      "00408.npz\n",
      "00410.npz\n",
      "00412.npz\n",
      "00414.npz\n",
      "00416.npz\n",
      "00418.npz\n",
      "00420.npz\n",
      "00422.npz\n",
      "00424.npz\n",
      "00426.npz\n",
      "00428.npz\n",
      "00430.npz\n",
      "00432.npz\n",
      "00434.npz\n",
      "00436.npz\n",
      "00438.npz\n",
      "00440.npz\n",
      "00442.npz\n",
      "00444.npz\n",
      "00446.npz\n",
      "00448.npz\n",
      "00450.npz\n",
      "00452.npz\n",
      "00454.npz\n",
      "00456.npz\n",
      "00458.npz\n",
      "00460.npz\n",
      "00462.npz\n",
      "00464.npz\n",
      "00466.npz\n",
      "00468.npz\n",
      "00470.npz\n",
      "00472.npz\n",
      "00474.npz\n",
      "00476.npz\n",
      "00478.npz\n",
      "00480.npz\n",
      "00482.npz\n",
      "00484.npz\n",
      "00486.npz\n",
      "00488.npz\n",
      "00490.npz\n",
      "00492.npz\n",
      "00494.npz\n",
      "00496.npz\n",
      "00498.npz\n",
      "00500.npz\n",
      "00502.npz\n",
      "00504.npz\n",
      "00506.npz\n",
      "00508.npz\n",
      "00510.npz\n",
      "00512.npz\n",
      "00514.npz\n",
      "00516.npz\n",
      "00518.npz\n",
      "00520.npz\n",
      "00522.npz\n",
      "00524.npz\n",
      "00526.npz\n",
      "00528.npz\n",
      "00530.npz\n",
      "00532.npz\n",
      "00534.npz\n",
      "00536.npz\n",
      "00538.npz\n",
      "00540.npz\n",
      "00542.npz\n",
      "00544.npz\n",
      "00546.npz\n",
      "00548.npz\n",
      "00550.npz\n",
      "00552.npz\n",
      "00554.npz\n",
      "00556.npz\n",
      "00558.npz\n",
      "00560.npz\n",
      "00562.npz\n",
      "00564.npz\n",
      "00566.npz\n",
      "00568.npz\n",
      "00570.npz\n",
      "00572.npz\n",
      "00574.npz\n",
      "00576.npz\n",
      "00578.npz\n",
      "00580.npz\n",
      "00582.npz\n",
      "00584.npz\n",
      "00586.npz\n",
      "00588.npz\n",
      "00590.npz\n",
      "00592.npz\n",
      "00594.npz\n",
      "00596.npz\n",
      "00598.npz\n",
      "00600.npz\n",
      "00602.npz\n",
      "00604.npz\n",
      "00606.npz\n",
      "00608.npz\n",
      "00610.npz\n",
      "00612.npz\n",
      "00614.npz\n",
      "00616.npz\n",
      "00618.npz\n",
      "00620.npz\n",
      "00622.npz\n",
      "00624.npz\n",
      "00626.npz\n",
      "00628.npz\n",
      "00630.npz\n",
      "00632.npz\n",
      "00634.npz\n",
      "00636.npz\n",
      "00638.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "path = 'data_animation/aist_demo/seqs'\n",
    "\n",
    "pose_list = []\n",
    "transl_list = []\n",
    "\n",
    "for file in sorted(os.listdir(path)):\n",
    "    print(file)\n",
    "    file_path = os.path.join(path, file)\n",
    "    # Load data\n",
    "    data = dict(np.load(file_path))\n",
    "    pose_list.append(data['pose'])\n",
    "    transl_list.append(data['transl'])\n",
    "\n",
    "summed_file = {\n",
    "    'pose': pose_list,\n",
    "    'transl': transl_list\n",
    "}\n",
    "\n",
    "np.save('data_animation/aist_poses.npy', summed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "frame_0 = np.load('data_animation/aist_poses.npy', allow_pickle=True).item()['pose'][0]\n",
    "frame_5 = np.load('data_animation/aist_poses.npy', allow_pickle=True).item()['pose'][5]\n",
    "frame_10 = np.load('data_animation/aist_poses.npy', allow_pickle=True).item()['pose'][10]\n",
    "frame_12 = np.load('data_animation/aist_poses.npy', allow_pickle=True).item()['pose'][12]\n",
    "frame_13 = np.load('data_animation/aist_poses.npy', allow_pickle=True).item()['pose'][13]\n",
    "frame_14 = np.load('data_animation/aist_poses.npy', allow_pickle=True).item()['pose'][14]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.smpl_paths import SmplPaths\n",
    "# unposed and unshaped smpl mesh (reference)\n",
    "ref_sp = SmplPaths(gender='male')\n",
    "ref_smpl = ref_sp.get_smpl()\n",
    "ref_smpl.pose[:], ref_smpl.betas[:], ref_smpl.trans[:] = frame_0, 0, 0\n",
    "smpl_skinning = ref_smpl.weights.r\n",
    "smpl_shape = ref_smpl.r\n",
    "smpl_shape.setflags(write=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# save point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(smpl_shape)\n",
    "o3d.io.write_point_cloud(\"visualization/smpl_shape.ply\", pcd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
